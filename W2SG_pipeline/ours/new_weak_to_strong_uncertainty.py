import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import datasets
from datasets import load_dataset
import argparse
import re
from tqdm import tqdm
import json
from vllm import LLM, SamplingParams
import os
from InstructorEmbedding import INSTRUCTOR
from sklearn.metrics.pairwise import cosine_similarity
import requests
from openai import OpenAI
import transformers
import gc
import vllm
from vllm.distributed.parallel_state import destroy_model_parallel
import jsonlines

if __name__ == "__main__":
    argparser = argparse.ArgumentParser()

    argparser.add_argument("--data")
    argparser.add_argument("--teacher")
    argparser.add_argument("--student")
    argparser.add_argument("--method")
    argparser.add_argument("--midlevel")
    argparser.add_argument("--model")

    args = argparser.parse_args()
    data = args.data
    teacher = args.teacher
    student = args.student
    method = args.method
    midlevel = args.midlevel
    model = args.model


    HF_token = 'xxxxxx'


    def batch_inference(modell, inputs):
        if modell == 'llama':
            tokenizer_path = './Meta-Llama-3-70B-Instruct'
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, token=HF_token)
            model_path = './Meta-Llama-3-70B-Instruct'
        llm = LLM(model=model_path, tokenizer=tokenizer_path, gpu_memory_utilization=0.5, tensor_parallel_size = 4)  
        batch_inputs = []
        for item in inputs:
            messages = [{'role': 'user', 'content': item}]
            batch_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
            )
            batch_inputs.append(batch_input)

        sampling_params = SamplingParams(temperature=0.9, top_p=0.95, max_tokens=4096)
        
        instructions = []
        results = []
        
        outputs = llm.generate(batch_inputs, sampling_params)
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            if modell == 'llama':
                instructions.append(prompt.split('<|start_header_id|>user<|end_header_id|>\n\n')[1].split('<|eot_id|>')[0])
            else:
                instructions.append(prompt.split('user\n')[1].split('<|im_end|>')[0])
            results.append(generated_text)
        
        return instructions, results






    template_to_generate_uncertainty = '''Your task is to analyze a question provided to you along with a set of correct and incorrect responses generated by my model. Your objective is to identify and summarize the inconsistency in the models' responses that can explain why my model is uncertain about the correct answer.

    Please note that:
    1. You should give the reasons from a first-person perspective, as if you are my model that gives the provided responses.
    2. Limit your explanation to the knowledge and facts the model possesses about the question.
    3. Keep your summary brief, aiming for 1-3 sentences.
    4. Please directly provide the summarized reason without any greetings or other unnecessary information. If you find all the responses are quite the same regarding the question, please directly return N/A. 
    5. Importantly!! My model only has access to one response at a time and it does not know what is correct and what is incorrect. Thus, the summary you provided should not include any statement like "My different responses have...", "my multiple responses about ...", etc. You should not say "my responses" or "the responses" anywhere in the summary. Just simply provide the uncertainty.

    Here is an example:
    Question: Sky High starred the actress who is married to which actor?

    Responses:
    1. The actress who starred in "Sky High" (2005) and is married to an actor is Kelly Preston. Her husband is John Travolta. The two have been married since 1991 and have three children together.
    2. The actress who starred in "Sky High" (2005) and is married to an actor is Kristen Bell. Bell voiced the main character, Layla, in "Sky High," and she is married to Dax Shepard, who is also an actor.
    3. The actress who starred in "Sky High" (2005) and is married to an actor is Kelly Clarkson. Her acting debut was in this film, and she married singer and actor Brandon Blackstock in 2013.

    The output can be: I am uncertain about the correct actress in "Sky High". There is a probability that the actress is Kristen Bell, instead of Kelly Preston. I am confused about her voice acting roles with on-screen appearances. There is also some probability that the actress is Kelly Clarkson.

    Now consier the following case:
    Question: {}

    Responses: 
    {}
    '''
    
    
    template_to_generate_uncertainty2 = '''Your task is to analyze a question provided to you along with a set of correct and incorrect responses generated by my model. Your objective is to identify and summarize the inconsistency in the models' responses that can explain why my model is uncertain about the correct answer.

    Please note that:
    1. You should give the reasons from a first-person perspective, as if you are my model that gives the provided responses.
    2. Limit your explanation to the knowledge and facts the model possesses about the question.
    3. Keep your summary brief, aiming for 1-3 sentences.
    4. Please directly provide the summarized reason without any greetings or other unnecessary information. If you find all the responses are quite the same regarding the question, please directly return N/A. 
    5. Importantly!! My model only has access to one response at a time and it does not know what is correct and what is incorrect. Thus, the summary you provided should not include any statement like "My different responses have...", "my multiple responses about ...", etc. You should not say "my responses" or "the responses" anywhere in the summary. Just simply provide the uncertainty.

    Here is an example:
    Question: A pen and pencil have a total cost of $6. If the pen costs twice as much as the pencil, what is the cost of the pen?
    
    Responses:
    1. Let x be the cost of the pencil.\nThe total cost of the pen and pencil is x + x = 6.\n2x = 6.\nThe cost of the pen is 6/2 = $<<6/2=3>>3.\n#### 3
    2. Let p be the cost of the pen and c be the cost of the pencil.\nWe know that p + c = 6\nWe also know that p = 2 * c\nSubstituting the second equation into the first equation, we get 2 * c + c = 6\nCombining like terms, we get 3 * c = 6\nDividing both sides by 3, we get c = <<3=2>>2\nSubstituting this value back into p + c = 6, we get p + 2 = 6\nSubtracting 2 from both sides, we get p = <<2=4>>4\n#### 4
    3. 
    
    The output can be: I am uncertain about how to formulate the cost of pen and pencil. There is a probability that the cost of pen and pencil are the same. But there is also a probability that the cost of the pen is twice as much as the pencil.

    Now consier the following case:
    Question: {}

    Responses: 
    {}
    '''


    def wrap_template(question, clustered):
        responses = ""
        for i, cluster in enumerate(clustered):
            response = cluster['response']
            responses  += f"{i+1}. {response}\n"

        if data == 'gsm8k':
            prompt = template_to_generate_uncertainty2.format(question, responses)
        else:    
            prompt = template_to_generate_uncertainty.format(question, responses)
        return prompt


    if method == 'cascade':
        with jsonlines.open('./data/ours_cascade_halfhalf/{}/{}_{}_{}_sampled_clustered.jsonl'.format(data,model,midlevel,data)) as reader:
            ds = [d for d in reader]
    else:
        with jsonlines.open('./data/ours/{}/{}_{}_{}_sampled_clustered.jsonl'.format(data,model,teacher,data)) as reader:
            ds = [d for d in reader]
   

    instruction_to_summarize_uncertainty = []
    to_get_new_responses = []
    for item in ds:
        if len(item['clusters']) == 1:
            # uncertainty = 'N/A'
            if method == 'cascade':
                with open('./data/ours_cascade_halfhalf/{}/{}_{}_to_{}_{}.jsonl'.format(data,model,midlevel, student, data), 'a') as f:
                    f.write(json.dumps({'question': item['question'], 'answer':item['final_response'], 'label':'original'}) + '\n')
            else:
                with open('./data/ours/{}/{}_{}_to_{}_{}.jsonl'.format(data,model,teacher, student, data), 'a') as f:
                    f.write(json.dumps({'question': item['question'], 'answer':item['final_response'], 'label':'original'}) + '\n')
        else:
            get_uncertainty_prompt = wrap_template(item['question'], item['clusters'])
            instruction_to_summarize_uncertainty.append(get_uncertainty_prompt)
            to_get_new_responses.append({'question': item['question'], 'final_response': item['final_response']})

    instruction_to_uncertainty, uncertainties = batch_inference('llama', instruction_to_summarize_uncertainty)
    for i, uncertainty in enumerate(uncertainties):
        to_get_new_responses[i]['uncertainty'] = uncertainty
        if method == 'cascade':
            with open('./data/ours_cascade_halfhalf/{}/{}_{}_{}_uncertainty.jsonl'.format(data,model,midlevel, data), 'a') as f:
                result = to_get_new_responses[i]
                f.write(json.dumps(result) + '\n')
        else:
            with open('./data/ours/{}/{}_{}_{}_uncertainty.jsonl'.format(data,model,teacher,data), 'a') as f:
                result = to_get_new_responses[i]
                f.write(json.dumps(result) + '\n')

    print('Uncertainties generated and saved.')



        
